{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Assignment 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1A: Anscombe's quartet\n",
    "\n",
    "\n",
    "Start by downloading these four datasets: [Data 1](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data1.tsv), [Data 2](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data2.tsv), [Data 3](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data3.tsv), and [Data 4](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data4.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "Each file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "It's ok to just download these files to disk by right-clicking on each one, but if you use Python and _urllib_ or _urllib2_ to get them, I'll really be impressed. If you don't know how to do that, I recommend opening up Google and typing \"download file using Python\" or something like that. When interpreting the search results remember that _stackoverflow_ is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os.path\n",
    "import urllib\n",
    "import csv\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "\n",
    "def download_file(url, file_name) :\n",
    "    #search for the file in the temp dir\n",
    "    tmp_dir = tempfile.gettempdir()\n",
    "    file_path = tmp_dir+\"/\"+file_name\n",
    "    if not os.path.isfile(file_path) :\n",
    "        urllib.urlretrieve(url, file_path)\n",
    "        print file_name+\" downloaded\"\n",
    "    print \"File \"+file_name+\" is ready\"\n",
    "    return file_path\n",
    "\n",
    "def read_data_file(file_path):\n",
    "    infile = open(file_path, 'r')    # open the file for reading\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    data_csv = []\n",
    "    # read through the CSV one line at a time\n",
    "    for i,line in enumerate(reader):\n",
    "        # assign the various fields in the line to variables\n",
    "        record = {}\n",
    "        record[\"x\"] = int(line[0])\n",
    "        record[\"y\"] = float(line[1])\n",
    "        #print record\n",
    "        data_csv.append(record)\n",
    "    print \"loaded: \"+(file_path.split(\"/\")[-1])\n",
    "    return data_csv\n",
    "\n",
    "data = []\n",
    "for i in range(1,5):\n",
    "    data_file = download_file(\"https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data%d.tsv\"%i,\n",
    "                              \"data%d.tsv\"%i)\n",
    "    data.append(read_data_file(data_file))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    "* Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,data_table in enumerate(data):\n",
    "    print \"data%d x mean: % 6.2f\"%(i,np.mean([row[\"x\"] for row in data_table]))\n",
    "    print \"data%d y mean: % 6.2f\"%(i,np.mean([row[\"y\"] for row in data_table]))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now calculate the variance for all of the various sets of $x$- and $y$-values (to three decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,data_table in enumerate(data):\n",
    "    print \"data%d x variance: % 6.3f\"%(i,np.var([row[\"x\"] for row in data_table]))\n",
    "    print \"data%d y variance: % 6.3f\"%(i,np.var([row[\"y\"] for row in data_table]))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also to three decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,data_table in enumerate(data):\n",
    "    x = [row[\"x\"] for row in data_table]\n",
    "    y = [row[\"y\"] for row in data_table]\n",
    "    \n",
    "    coef = np.corrcoef(x,y)\n",
    "    print \"data%d Pearson correlation: % 6.3f\"%(i,coef[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "```\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "slope_regress = []\n",
    "for i,data_table in enumerate(data):\n",
    "    x = [row[\"x\"] for row in data_table]\n",
    "    y = [row[\"y\"] for row in data_table]\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    slope_regress.append([slope,intercept])\n",
    "    print \"data%d slope: % 6.2f, intercept: % 6.2f\"%(i, slope, intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/anscombe.png).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.close('all')\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')\n",
    "ax = []\n",
    "ax.append(ax1)\n",
    "ax.append(ax2)\n",
    "ax.append(ax3)\n",
    "ax.append(ax4)\n",
    "ax1.set_title('data with linear fit')\n",
    "for i,data_table in enumerate(data):\n",
    "    x = [row[\"x\"] for row in data_table]\n",
    "    y = [row[\"y\"] for row in data_table]\n",
    "    ax[i].scatter(x, y)\n",
    "    X_plot = np.linspace(0,25,20)\n",
    "    m=slope_regress[i][0]\n",
    "    b=slope_regress[i][1]\n",
    "    ax[i].plot(X_plot, m*X_plot + b, color='r')\n",
    "    ax[i].set_xlim([3, 21])\n",
    "    ax[i].set_ylim([2, 14])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain - in your own words - what you think my point with this exercise is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1B: Slicing data\n",
    "\n",
    "We investigate the types of crime and how they take place across San Francisco's police districts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os.path\n",
    "import urllib\n",
    "import csv\n",
    "import numpy\n",
    "import dateutil.parser\n",
    "#search for the file in the temp dir\n",
    "file_path = download_file('https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD',\n",
    "                          \"SFPD_Incidents_-_from_1_January_2003.csv\")\n",
    "\n",
    "data = []\n",
    "# read through the CSV one line at a time\n",
    "with open(file_path, 'r') as infile:\n",
    "    reader = csv.DictReader(infile, delimiter=',')\n",
    "    for line in reader:\n",
    "        # assign the various fields in the line to variables\n",
    "        crime = {}\n",
    "        crime[\"category\"] = line[\"Category\"]\n",
    "        crime[\"time\"] = dateutil.parser.parse(line[\"Date\"]+\" \"+line[\"Time\"])\n",
    "        crime[\"neighborhood\"] = line[\"PdDistrict\"]\n",
    "        crime[\"latitude\"], crime[\"longitude\"] = eval(line[\"Location\"])\n",
    "        if crime[\"neighborhood\"] != None:\n",
    "            data.append(crime)\n",
    "        else:\n",
    "            print \"crime with errors: \"\n",
    "            print crime\n",
    "print \"File loaded\"\n",
    "\n",
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE',\n",
    "                  'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC',\n",
    "                  'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY',\n",
    "                  'DISORDERLY CONDUCT'])\n",
    "\n",
    "data_focuscrimes = filter(lambda a: a['category'] in focuscrimes, data)\n",
    "\n",
    "print len(data), len(data_focuscrimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We'll be combining information about _PdDistrict_ and _Category_ to explore differences between SF's neighborhoods. First, simply list the names of SF's 10 police districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts = set(map(lambda a: a['neighborhood'], data))\n",
    "\n",
    "# removes the empty string\n",
    "districts = filter(None, districts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which has the most crimes? Which has the most focus crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimes_per_distrcit = {}\n",
    "No_crimes_per_distrcit = {}\n",
    "No_focuscrimes_per_distrcit = {}\n",
    "for district in districts:\n",
    "    crimes_per_distrcit[district] = filter(lambda a: a['neighborhood'] == district, data)\n",
    "    No_crimes_per_distrcit[district] = len(crimes_per_distrcit[district])\n",
    "    No_focuscrimes_per_distrcit[district] = len(filter(lambda a: a['category'] in focuscrimes,\n",
    "                                             crimes_per_distrcit[district]))\n",
    "    \n",
    "print \"District with most crimes: \" + sorted(\n",
    "    No_crimes_per_distrcit, key=No_crimes_per_distrcit.get, reverse=True)[0]\n",
    "print \"District with most focus crimes: \" + sorted(\n",
    "    No_focuscrimes_per_distrcit, key=No_focuscrimes_per_distrcit.get, reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we want to generate a slightly more complicated graphic. I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical. Below I describe how to get that plot going\n",
    "- First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole. That's simply a normalized version of [this plot](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/categoryhist.png). Let's call it `P(crime)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "from collections import Counter\n",
    "categories = [item['category'] for item in data_focuscrimes]\n",
    "\n",
    "c_cat = Counter(categories)\n",
    "p_crimes = {}\n",
    "for category, times in c_cat.items():\n",
    "    p_crimes[category] = times / len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we calculate that same probability distribution _but for each PD district_, let's call that `P(crime|district)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_crime_district = {}\n",
    "for district, crimes in crimes_per_distrcit.items():\n",
    "    fcrimes = filter(lambda a: a['category'] in focuscrimes, crimes)\n",
    "    district_categories = [item['category'] for item in fcrimes]\n",
    "    c_dis_cat = Counter(district_categories)\n",
    "    p_district_crimes = {}\n",
    "    \n",
    "    for category, times in c_dis_cat.items():\n",
    "        p_district_crimes[category] = times / len(crimes)\n",
    "\n",
    "    p_crime_district[district] = p_district_crimes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we look at the ratio `P(crime|district)/P(crime)`. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs _more frequently_ within that district. If it's smaller than one, it means that the crime is _rarer within the district in question_ than in the city as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_ratio = {}\n",
    "\n",
    "for district, cat_probs in p_crime_district.items():\n",
    "    ratio_cat_probs = {}\n",
    "    for cat, prob in cat_probs.items():\n",
    "        ratio = prob / p_crimes[cat]\n",
    "        ratio_cat_probs[cat] = ratio\n",
    "    p_ratio[district] = ratio_cat_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each district plot these ratios for the 14 focus crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "width = .7\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, sharex=True, figsize=(16,12))\n",
    "indx = 0\n",
    "indy = 0\n",
    "for district, cat_probs in p_ratio.items(): \n",
    "    ax[indx][indy].bar([i + 0.5 for i in range(len(cat_probs.values()))],\n",
    "                       cat_probs.values(), width)\n",
    "    ax[indx][indy].set_xlim([0,15])\n",
    "    ax[indx][indy].set_ylabel('ratio')\n",
    "    ax[indx][indy].set_title(district, y=0.8, x=0.15)\n",
    "    ax[indx][indy].set_xticks([])\n",
    "    ax[indx][indy].set_yticks([i / 2 for i in range(9)])\n",
    "    ax[indx][indy].set_yticklabels([i / 2 for i in range(9)])\n",
    "    indx += 1\n",
    "    if indx == 5:\n",
    "        ax[indx-1][indy].set_xticks(range(1, len(cat_probs.keys()) + 1))\n",
    "        ax[indx-1][indy].set_xticklabels(cat_probs.keys(), rotation=90)\n",
    "        indx = 0\n",
    "        indy += 1\n",
    "        \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   - Comment on the top crimes in _Tenderloin_, _Mission_, and _Richmond_. Does this fit with the impression you get of these neighborhoods on Wikipedia?\n",
    "   \n",
    "The top crimes in _Tenderloin_ are Trespass, Weapons laws and Assault. For _Mission_ the top crimes are Prostitution, Weapons law. Finally for _Richmond_ the top crimes are Driving under influence and Drugs/narcotics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though we only plotted the ratios for our 14 focus crimes, I asked you to calculate the ratios based on all crime categories. Why do you think I wanted to include all crime types in the calculation?\n",
    "\n",
    "This way the propabilities we calculate take into account the overal crime activity in the district. If a crime not in focuscrimes is very frequent in the district the propabilities of focuscrimes would have been way smaller. If we did not do that we could have produced missleading conclusions. For example if kidnapping is the most frequent crime in a district with a frequency lets say of 10 followed by prostitution with a frequency of 3 and assault with frequency of 1, it would have been a mistake to consider the propabilities of prostitution 75% and of assault 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1C: KNN\n",
    "\n",
    "\n",
    "The goal of this exercise is to create a useful real-world version of the example on pp153 in DSFS. We know from last week's exercises that the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE` tend to be concentrated in certain neighborhoods, so we focus on those crime types since they will make the most sense a KNN - map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lat = [item[\"latitude\"] for item in data]\n",
    "lon = [item[\"longitude\"] for item in data]\n",
    "geo_data_for_plotting = {\"lat\": lat,\n",
    "                         \"lon\": lon}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Begin by using `geoplotlib` to plot all incidents of the three crime types on their own map using [`geoplotlib.kde()`](https://github.com/andrea-cuttone/geoplotlib/blob/master/examples/kde.py). This will give you an idea of how the varioius crimes are distributed across the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geoplotlib\n",
    "from geoplotlib.utils import BoundingBox, DataAccessObject\n",
    "geoplotlib.kde(geo_data_for_plotting, bw=5, cut_below=1e-4, alpha=150)\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, it's time to set up your model based on the actual data. You can use the code supplied in the book or try out `scikit-learn`'s [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). If you end up using the latter (recommended), you may want to check out [this example](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html) to get a sense of the usage.\n",
    "  - You don't have to think a lot about testing/trainig and accuracy for this exercise. We're mostly interested in creating a map that's not too problematic. **But** do calculate the number of observations of each crime-type respectively. You'll find that the levels of each crime varies (lots of drug arrests, an intermediate amount of prostitiution registered, and very little drunk driving in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions from the book to split the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x, y) # pair corresponding values\n",
    "    train, test = split_data(data, 1 - test_pct) # split the data set of pairs\n",
    "    x_train, y_train = zip(*train) # magical un-zip trick\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "focus_categories = ['PROSTITUTION', 'DRUG/NARCOTIC', 'DRIVING UNDER THE INFLUENCE']\n",
    "\n",
    "focus_crimes = filter(lambda a: a['category'] in focus_categories, data)\n",
    "\n",
    "lat = [item[\"latitude\"] for item in focus_crimes]\n",
    "lon = [item[\"longitude\"] for item in focus_crimes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take a third of the data to test and the resto to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gps_cords = zip(lat,lon)\n",
    "categories = [item[\"category\"] for item in focus_crimes]\n",
    "x_train, x_test, y_train, y_test = train_test_split(gps_cords, categories, 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors, datasets\n",
    "knn=neighbors.KNeighborsClassifier()\n",
    "knn.fit(gps_cords, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "y_predicted = knn.predict(x_test)\n",
    "\n",
    "lat = [item[0] for item in x_test]\n",
    "lon = [item[1] for item in x_test]\n",
    "\n",
    "geo_data_for_plotting = {\"lat\": lat,\n",
    "                         \"lon\": lon}\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(focus_categories)))\n",
    "cat_colors = []\n",
    "\n",
    "for i, cat in enumerate(y_predicted) :\n",
    "    cat_colors.append(colors[ focus_categories.index(cat) ])\n",
    "\n",
    "for i, gps in enumerate(x_test):\n",
    "    geo_data_for_plotting = {\"lat\": [gps[0]],\n",
    "                         \"lon\": [gps[1]]}\n",
    "    if y_predicted[i]==\"PROSTITUTION\":\n",
    "        geoplotlib.dot(geo_data_for_plotting, \"red\")\n",
    "    elif y_predicted[i]==\"DRUG/NARCOTIC\":\n",
    "        geoplotlib.dot(geo_data_for_plotting, \"blue\")\n",
    "    else:#DRIVING UNDER THE INFLUENCE\n",
    "        geoplotlib.dot(geo_data_for_plotting, \"green\")\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Since the algorithm classifies each point according to it's neighbors, what could a consequence of this imbalance in the number of examples from each class mean for your map?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - You can make the dataset 'balanced' by grabbing an equal number of examples from each crime category. How do you expect that will change the KNN result? In which situations is the balanced map useful - and when is the map that data in proportion to occurrences useful? Choose which map you will work on in the following. \n",
    "  \n",
    "* I expect to visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "data_cat = [item[\"category\"] for item in focus_crimes]\n",
    "categories_count = Counter(data_cat)\n",
    "min_cat_count = min(categories_count.get('DRUG/NARCOTIC'), categories_count.get('PROSTITUTION'), categories_count.get('DRIVING UNDER THE INFLUENCE'))\n",
    "sampled_crimes = []\n",
    "for category in focus_categories:\n",
    "    cat_crimes = filter(lambda a: a['category'] == category, focus_crimes)\n",
    "    sampled_crimes.extend(random.sample(cat_crimes, min_cat_count))\n",
    "\n",
    "lat = [item[\"latitude\"] for item in sampled_crimes]\n",
    "lon = [item[\"longitude\"] for item in sampled_crimes]\n",
    "\n",
    "gps_cords = zip(lat,lon)\n",
    "categories = [item[\"category\"] for item in sampled_crimes]\n",
    "x_train, x_test, y_train, y_test = train_test_split(gps_cords, categories, 0.33)\n",
    "\n",
    "knn=neighbors.KNeighborsClassifier()\n",
    "knn.fit(gps_cords, categories)\n",
    "\n",
    "y_predicted = knn.predict(x_test)\n",
    "#y_predicted = y_test\n",
    "\n",
    "lat = [item[0] for item in x_test]\n",
    "lon = [item[1] for item in x_test]\n",
    "\n",
    "geo_data_for_plotting = {\"lat\": lat,\n",
    "                         \"lon\": lon}\n",
    "\n",
    "colors = cm.rainbow(np.linspace(0, 1, len(focus_categories)))\n",
    "cat_colors = []\n",
    "\n",
    "for i, cat in enumerate(y_predicted) :\n",
    "    cat_colors.append(colors[ focus_categories.index(cat) ])\n",
    "\n",
    "for i, gps in enumerate(x_test):\n",
    "    geo_data_for_plotting = {\"lat\": [gps[0]],\n",
    "                         \"lon\": [gps[1]]}\n",
    "    if y_predicted[i]==\"PROSTITUTION\":\n",
    "        geoplotlib.dot(geo_data_for_plotting, \"red\")\n",
    "    elif y_predicted[i]==\"DRUG/NARCOTIC\":\n",
    "        geoplotlib.dot(geo_data_for_plotting, \"blue\")\n",
    "    else:#DRIVING UNDER THE INFLUENCE\n",
    "        geoplotlib.dot(geo_data_for_plotting, \"green\")\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now create an approximately square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $50 \\times 50$ and $100 \\times 100$ points. I recommend plotting using `geoplotlib.dot()`. \n",
    "* Visualize your model by coloring the grid, coloring each grid point according to it's category. Create a plot of this kind for models where each point is colored according to the majority of its 5, 10, and 30 nearest neighbors. Describe what happens to the map as you increase the number of neighbors, `K`.  \n",
    "\n",
    "**NOTE**: To get a map only of SF, you need to create your own * BoundingBox * which can be done in the following way:\n",
    "```\n",
    "bbox = BoundingBox(north=max_lat, west=min_lon, south=min_lat, east=max_lon)\n",
    "geoplotlib.set_bbox(bbox)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1D: Multiple regression and the Red Baron\n",
    "\n",
    "Investigate Chief Suneman's idea is that the Red Baron might pick the time of his attacks according to a pattern that we can detect using the powers of data science.\n",
    "\n",
    "If he's right, we can identify the time of the next attack, which will help us end this insanity once and for all. Well, let's see if he is right!\n",
    "\n",
    "* Start from all cases having `Red Baron` in the resolution field and use the day of the week to predict the hour of the day when he is attacking, e.g. use **linear regression** to infer the hour of the day based on the weekday! Again, take 4/5 of the data for training and then calculate goodness of fit using $R^2$ on the rest 1/5. Don't forget to rescale your input variables! (Note 1: My goodness of fit after using the weekdays is only around 0.618). (Note 2: For multivariate regression, as always you can simply re-use the code in the DSFS book (Chapters 14-15) or scikit-learn).\n",
    "* Now, add the crime year as well to the input variables! Did the goodness of fit improve? (Note: Mine did to 0.809)\n",
    "* It is still low. Inspired by a movie he once watched, Chief Suneman yells: \"Let's add the longitude of the crimes as well!\" Is your prediction getting better? (It should, to around 0.993)\n",
    "* Very nice! Why not add latitude as well? What do you find now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
