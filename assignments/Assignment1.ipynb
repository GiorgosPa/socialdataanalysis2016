{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1A: Anscombe's quartet\n",
    "\n",
    "\n",
    "Start by downloading these four datasets: [Data 1](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data1.tsv), [Data 2](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data2.tsv), [Data 3](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data3.tsv), and [Data 4](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data4.tsv). The format is `.tsv`, which stands for _tab separated values_. \n",
    "Each file has two columns (separated using the tab character). The first column is $x$-values, and the second column is $y$-values.  \n",
    "\n",
    "It's ok to just download these files to disk by right-clicking on each one, but if you use Python and _urllib_ or _urllib2_ to get them, I'll really be impressed. If you don't know how to do that, I recommend opening up Google and typing \"download file using Python\" or something like that. When interpreting the search results remember that _stackoverflow_ is your friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os.path\n",
    "import urllib\n",
    "import csv\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "\n",
    "def download_file(url, file_name) :\n",
    "    #search for the file in the temp dir\n",
    "    tmp_dir = tempfile.gettempdir()\n",
    "    file_path = tmp_dir+\"/\"+file_name\n",
    "    #If the file is not found, download it\n",
    "    if not os.path.isfile(file_path) :\n",
    "        urllib.urlretrieve(url, file_path)\n",
    "        print file_name+\" downloaded\"\n",
    "    print \"File \"+file_name+\" is ready\"\n",
    "    return file_path\n",
    "\n",
    "def read_data_file(file_path):\n",
    "    infile = open(file_path, 'r')    # open the file for reading\n",
    "    reader = csv.reader(infile, delimiter='\\t')\n",
    "    data_csv = []\n",
    "    # read through the CSV one line at a time\n",
    "    for i,line in enumerate(reader):\n",
    "        # assign the various fields in the line to variables\n",
    "        record = {}\n",
    "        record[\"x\"] = int(line[0])\n",
    "        record[\"y\"] = float(line[1])\n",
    "        data_csv.append(record)\n",
    "    print \"loaded: \"+(file_path.split(\"/\")[-1])\n",
    "    return data_csv\n",
    "\n",
    "data = []\n",
    "for i in range(1,5):\n",
    "    data_file = download_file(\"https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/data%d.tsv\"%i,\n",
    "                              \"data%d.tsv\"%i)\n",
    "    data.append(read_data_file(data_file))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the `numpy` function `mean`, calculate the mean of both $x$-values and $y$-values for each dataset. \n",
    "* Use python string formatting to print precisely two decimal places of these results to the output cell. Check out [this _stackoverflow_ page](http://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python) for help with the string formatting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,data_table in enumerate(data):\n",
    "    print \"data%d x mean: % 6.2f\"%(i,np.mean([row[\"x\"] for row in data_table]))\n",
    "    print \"data%d y mean: % 6.2f\"%(i,np.mean([row[\"y\"] for row in data_table]))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now calculate the variance for all of the various sets of $x$- and $y$-values (to three decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,data_table in enumerate(data):\n",
    "    print \"data%d x variance: % 6.3f\"%(i,np.var([row[\"x\"] for row in data_table]))\n",
    "    print \"data%d y variance: % 6.3f\"%(i,np.var([row[\"y\"] for row in data_table]))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `numpy` to calculate the [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient) between $x$- and $y$-values for all four data sets (also to three decimal places)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i,data_table in enumerate(data):\n",
    "    x = [row[\"x\"] for row in data_table]\n",
    "    y = [row[\"y\"] for row in data_table]\n",
    "    \n",
    "    coef = np.corrcoef(x,y)\n",
    "    print \"data%d Pearson correlation: % 6.3f\"%(i,coef[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The next step is use _linear regression_ to fit a straight line $f(x) = a x + b$ through each dataset and report $a$ and $b$ (to two decimal places). An easy way to fit a straight line in Python is using `scipy`'s `linregress`. It works like this\n",
    "```\n",
    "from scipy import stats\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "slope_regress = []\n",
    "for i,data_table in enumerate(data):\n",
    "    x = [row[\"x\"] for row in data_table]\n",
    "    y = [row[\"y\"] for row in data_table]\n",
    "    \n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)\n",
    "    slope_regress.append([slope,intercept])\n",
    "    print \"data%d slope: % 6.2f, intercept: % 6.2f\"%(i, slope, intercept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finally, it's time to plot the four datasets using `matplotlib.pyplot`. Use a two-by-two [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html) to put all of the plots nicely in a grid and use the same $x$ and $y$ range for all four plots. And include the linear fit in all four plots. (To get a sense of what I think the plot should look like, you can take a look at my version [here](https://dl.dropboxusercontent.com/u/153071/teaching/02806_2016/anscombe.png).)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.close('all')\n",
    "#setting up the subplots layout \n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey='row')\n",
    "ax = []\n",
    "ax.append(ax1)\n",
    "ax.append(ax2)\n",
    "ax.append(ax3)\n",
    "ax.append(ax4)\n",
    "ax1.set_title('data with linear fit')\n",
    "#plot an scatter plot in each subplot\n",
    "for i,data_table in enumerate(data):\n",
    "    x = [row[\"x\"] for row in data_table]\n",
    "    y = [row[\"y\"] for row in data_table]\n",
    "    #plotting the scatter plot for the raw data in each dataset\n",
    "    ax[i].scatter(x, y)\n",
    "    X_plot = np.linspace(0,25,20)\n",
    "    m=slope_regress[i][0]\n",
    "    b=slope_regress[i][1]\n",
    "    #ploting the linear regression with the previously calculated values\n",
    "    ax[i].plot(X_plot, m*X_plot + b, color='r')\n",
    "    ax[i].set_xlim([3, 21])\n",
    "    ax[i].set_ylim([2, 14])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Explain - in your own words - what you think my point with this exercise is.\n",
    "  * In this exercise we made an exploratory analysis of 4 datasets that are not so similar in their x vs. y Relation, nonetheless the measurements taken over this data gave almost the same results.\n",
    "  * If we look only at the measurements (mean, variance, Pearson correlation, linear regression, we could mistakenly conclude that this datasets behave the same, but when looking at the plotted data we realize that they are very different and itâ€™s just because some special features of the datasets that they seem to behave as this measurements indicate.\n",
    "\n",
    "  * For example in the datasets 3 and 4 its just 1 point what makes the linear regression behave the same that in the rest of the datasets, and in the file 2 we can see (judging from the shape) that a line would probably not be the best way to approximate this values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1B: Slicing data\n",
    "\n",
    "We investigate the types of crime and how they take place across San Francisco's police districts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import os.path\n",
    "import urllib\n",
    "import csv\n",
    "import numpy\n",
    "import dateutil.parser\n",
    "#search for the file in the temp dir\n",
    "file_path = download_file('https://data.sfgov.org/api/views/tmnf-yvry/rows.csv?accessType=DOWNLOAD',\n",
    "                          \"SFPD_Incidents_-_from_1_January_2003.csv\")\n",
    "\n",
    "data = []\n",
    "# read through the CSV one line at a time\n",
    "with open(file_path, 'r') as infile:\n",
    "    reader = csv.DictReader(infile, delimiter=',')\n",
    "    for line in reader:\n",
    "        # assign the various fields in the line to variables\n",
    "        crime = {}\n",
    "        crime[\"category\"] = line[\"Category\"]\n",
    "        crime[\"time\"] = dateutil.parser.parse(line[\"Date\"]+\" \"+line[\"Time\"])\n",
    "        crime[\"neighborhood\"] = line[\"PdDistrict\"]\n",
    "        crime[\"latitude\"], crime[\"longitude\"] = eval(line[\"Location\"])\n",
    "        if crime[\"neighborhood\"] is not '':\n",
    "            data.append(crime)\n",
    "        else:\n",
    "            print \"crime with errors: \"\n",
    "            print crime\n",
    "print \"File loaded\"\n",
    "\n",
    "focuscrimes = set(['WEAPON LAWS', 'PROSTITUTION', 'DRIVING UNDER THE INFLUENCE',\n",
    "                  'ROBBERY', 'BURGLARY', 'ASSAULT', 'DRUNKENNESS', 'DRUG/NARCOTIC',\n",
    "                  'TRESPASS', 'LARCENY/THEFT', 'VANDALISM', 'VEHICLE THEFT', 'STOLEN PROPERTY',\n",
    "                  'DISORDERLY CONDUCT'])\n",
    "\n",
    "data_focuscrimes = filter(lambda a: a['category'] in focuscrimes, data)\n",
    "\n",
    "print len(data), len(data_focuscrimes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* We'll be combining information about _PdDistrict_ and _Category_ to explore differences between SF's neighborhoods. First, simply list the names of SF's 10 police districts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts = set(map(lambda a: a['neighborhood'], data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Which has the most crimes? Which has the most focus crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crimes_per_distrcit = {}\n",
    "No_crimes_per_distrcit = {}\n",
    "No_focuscrimes_per_distrcit = {}\n",
    "for district in districts:\n",
    "    crimes_per_distrcit[district] = filter(lambda a: a['neighborhood'] == district, data)\n",
    "    No_crimes_per_distrcit[district] = len(crimes_per_distrcit[district])\n",
    "    No_focuscrimes_per_distrcit[district] = len(filter(lambda a: a['category'] in focuscrimes,\n",
    "                                             crimes_per_distrcit[district]))\n",
    "    \n",
    "print \"District with most crimes: \" + sorted(\n",
    "    No_crimes_per_distrcit, key=No_crimes_per_distrcit.get, reverse=True)[0]\n",
    "print \"District with most focus crimes: \" + sorted(\n",
    "    No_focuscrimes_per_distrcit, key=No_focuscrimes_per_distrcit.get, reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, we want to generate a slightly more complicated graphic. I'm interested to know if there are certain crimes that happen much more in certain neighborhoods than what's typical. Below I describe how to get that plot going\n",
    "- First, we need to calculate the relative probabilities of seeing each type of crime in the dataset as a whole. That's simply a normalized version of [this plot](https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/categoryhist.png). Let's call it `P(crime)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division \n",
    "from collections import Counter\n",
    "categories = [item['category'] for item in data_focuscrimes]\n",
    "\n",
    "c_cat = Counter(categories)\n",
    "p_crimes = {}\n",
    "for category, times in c_cat.items():\n",
    "    p_crimes[category] = times / len(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Next, we calculate that same probability distribution _but for each PD district_, let's call that `P(crime|district)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_crime_district = {}\n",
    "for district, crimes in crimes_per_distrcit.items():\n",
    "    fcrimes = filter(lambda a: a['category'] in focuscrimes, crimes)\n",
    "    district_categories = [item['category'] for item in fcrimes]\n",
    "    c_dis_cat = Counter(district_categories)\n",
    "    p_district_crimes = {}\n",
    "    \n",
    "    for category, times in c_dis_cat.items():\n",
    "        p_district_crimes[category] = times / len(crimes)\n",
    "\n",
    "    p_crime_district[district] = p_district_crimes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we look at the ratio `P(crime|district)/P(crime)`. That ratio is equal to 1 if the crime occurs at the same level within a district as in the city as a whole. If it's greater than one, it means that the crime occurs _more frequently_ within that district. If it's smaller than one, it means that the crime is _rarer within the district in question_ than in the city as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_ratio = {}\n",
    "\n",
    "for district, cat_probs in p_crime_district.items():\n",
    "    ratio_cat_probs = {}\n",
    "    for cat, prob in cat_probs.items():\n",
    "        ratio = prob / p_crimes[cat]\n",
    "        ratio_cat_probs[cat] = ratio\n",
    "    p_ratio[district] = ratio_cat_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each district plot these ratios for the 14 focus crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "width = .7\n",
    "\n",
    "fig, ax = plt.subplots(5, 2, sharex=True, figsize=(16,12))\n",
    "indx = 0\n",
    "indy = 0\n",
    "for district, cat_probs in p_ratio.items(): \n",
    "    ax[indx][indy].bar([i + 0.5 for i in range(len(cat_probs.values()))],\n",
    "                       cat_probs.values(), width)\n",
    "    ax[indx][indy].set_xlim([0,15])\n",
    "    ax[indx][indy].set_ylabel('ratio')\n",
    "    ax[indx][indy].set_title(district, y=0.8, x=0.15)\n",
    "    ax[indx][indy].set_xticks([])\n",
    "    ax[indx][indy].set_yticks([i / 2 for i in range(9)])\n",
    "    ax[indx][indy].set_yticklabels([i / 2 for i in range(9)])\n",
    "    indx += 1\n",
    "    if indx == 5:\n",
    "        ax[indx-1][indy].set_xticks(range(1, len(cat_probs.keys()) + 1))\n",
    "        ax[indx-1][indy].set_xticklabels(cat_probs.keys(), rotation=90)\n",
    "        indx = 0\n",
    "        indy += 1\n",
    "        \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   - Comment on the top crimes in _Tenderloin_, _Mission_, and _Richmond_. Does this fit with the impression you get of these neighborhoods on Wikipedia?\n",
    "   \n",
    "The top crimes in _Tenderloin_ are Trespass, Weapons laws and Assault. The wikipedia page for _Tenderloin_ states pretty much the same information, it is an area of high crime activity  particularly violent street crime such as robbery and aggravated assault. The top crimes for _Mission_ are Prostitution and Weapons law. _Mission_ is mainly inhabeted by imigrants and refuges from Latin America, which it could possible explain the prostitution and weapon laws crimes. Finally for _Richmond_ the top crimes are Driving under influence and Drugs/narcotics. _Richmond_ on the other half is a rich district with big mansions, so one would except these kind of crimes to be more frequent in a neighborhood like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Even though we only plotted the ratios for our 14 focus crimes, I asked you to calculate the ratios based on all crime categories. Why do you think I wanted to include all crime types in the calculation?\n",
    "\n",
    "This way the propabilities we calculate take into account the overal crime activity in the district. If a crime not in focuscrimes is very frequent in the district the propabilities of focuscrimes would have been way smaller. If we did not do that we could have produced missleading conclusions. For example if kidnapping is the most frequent crime in a district with a frequency lets say of 10 followed by prostitution with a frequency of 3 and assault with frequency of 1, it would have been a mistake to consider the propabilities of prostitution 75% and of assault 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1C: KNN\n",
    "\n",
    "\n",
    "The goal of this exercise is to create a useful real-world version of the example on pp153 in DSFS. We know from last week's exercises that the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE` tend to be concentrated in certain neighborhoods, so we focus on those crime types since they will make the most sense a KNN - map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Begin by using `geoplotlib` to plot all incidents of the three crime types on their own map using [`geoplotlib.kde()`](https://github.com/andrea-cuttone/geoplotlib/blob/master/examples/kde.py). This will give you an idea of how the varioius crimes are distributed across the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#geoplotlib requires the geo data in this format: \n",
    "lat = [item[\"latitude\"] for item in data]\n",
    "lon = [item[\"longitude\"] for item in data]\n",
    "geo_data_for_plotting = {\"lat\": lat,\n",
    "                         \"lon\": lon}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import geoplotlib\n",
    "from geoplotlib.utils import BoundingBox, DataAccessObject\n",
    "#Ploting the graph for density of the crimes in the city \n",
    "geoplotlib.kde(geo_data_for_plotting, bw=5, cut_below=1e-4, alpha=150)\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, it's time to set up your model based on the actual data. You can use the code supplied in the book or try out `scikit-learn`'s [`KNeighborsClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). If you end up using the latter (recommended), you may want to check out [this example](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html) to get a sense of the usage.\n",
    "  - You don't have to think a lot about testing/trainig and accuracy for this exercise. We're mostly interested in creating a map that's not too problematic. **But** do calculate the number of observations of each crime-type respectively. You'll find that the levels of each crime varies (lots of drug arrests, an intermediate amount of prostitiution registered, and very little drunk driving in the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the data for the focus crimes `PROSTITUTION`, `DRUG/NARCOTIC` and `DRIVING UNDER THE INFLUENCE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "focus_categories = ['PROSTITUTION', 'DRUG/NARCOTIC', 'DRIVING UNDER THE INFLUENCE']\n",
    "focus_crimes = filter(lambda a: a['category'] in focus_categories, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - Since the algorithm classifies each point according to it's neighbors, what could a consequence of this imbalance in the number of examples from each class mean for your map?\n",
    "    - As we can see below, there are much more drug related crimes in our dataset, this could make the prediction model biased towards predicting drug related crimes in most of the places instead of finding a the correlation between the coordinates and the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm\n",
    "from collections import Counter\n",
    "data_cat = [item[\"category\"] for item in focus_crimes]\n",
    "#counts the occurrences of each category\n",
    "categories_count = Counter(data_cat)\n",
    "print \"DRUG/NARCOTIC count: %d\"%categories_count.get('DRUG/NARCOTIC')\n",
    "print \"PROSTITUTION count: %d\"%categories_count.get('PROSTITUTION')\n",
    "print \"DRIVING UNDER THE INFLUENCE count: %d\"%categories_count.get('DRIVING UNDER THE INFLUENCE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - You can make the dataset 'balanced' by grabbing an equal number of examples from each crime category. How do you expect that will change the KNN result? In which situations is the balanced map useful - and when is the map that data in proportion to occurrences useful? Choose which map you will work on in the following. \n",
    "  \n",
    "    - Balancing the data for each category would find where is most likely for each crime to occur instead of just finding the most probable crime in general (which is in this case DRUG/NARCOTIC, so the sampled data is more useful to find this correlation between place and category.\n",
    "    - Taking into account all the occurrences of the 3 categories instead of balancing them could be useful for visualizing the rate between this 3 crimes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now create an approximately square grid of point that runs over SF. You get to decide the grid-size, but I recommend somewhere between $50 \\times 50$ and $100 \\times 100$ points. I recommend plotting using `geoplotlib.dot()`. \n",
    "* Visualize your model by coloring the grid, coloring each grid point according to it's category. Create a plot of this kind for models where each point is colored according to the majority of its 5, 10, and 30 nearest neighbors. Describe what happens to the map as you increase the number of neighbors, `K`.  \n",
    "\n",
    "**NOTE**: To get a map only of SF, you need to create your own * BoundingBox * which can be done in the following way:\n",
    "```\n",
    "bbox = BoundingBox(north=max_lat, west=min_lon, south=min_lat, east=max_lon)\n",
    "geoplotlib.set_bbox(bbox)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn import neighbors\n",
    "import random\n",
    "import geoplotlib\n",
    "from geoplotlib.utils import BoundingBox\n",
    "data_cat = [item[\"category\"] for item in focus_crimes]\n",
    "categories_count = Counter(data_cat)\n",
    "#we obtain the minimum number of crimes we can get from all the categories at the same time \n",
    "min_cat_count = min(categories_count.get('DRUG/NARCOTIC'),\n",
    "                    categories_count.get('PROSTITUTION'),\n",
    "                    categories_count.get('DRIVING UNDER THE INFLUENCE'))\n",
    "\n",
    "sampled_crimes = []\n",
    "#and then we randomly select that amount of crimes for each category\n",
    "for category in focus_categories:\n",
    "    cat_crimes = filter(lambda a: a['category'] == category, focus_crimes)\n",
    "    sampled_crimes.extend(random.sample(cat_crimes, min_cat_count))\n",
    "\n",
    "lat = [item[\"latitude\"] for item in sampled_crimes]\n",
    "lon = [item[\"longitude\"] for item in sampled_crimes]\n",
    "\n",
    "gps_cords = zip(lat,lon)\n",
    "categories = [item[\"category\"] for item in sampled_crimes]\n",
    "\n",
    "#setting up of equally spaced 100 points in the boundaries of SF\n",
    "longs = np.linspace(-122.37, -122.52, num=100)\n",
    "lats = np.linspace(37.7, 37.81, num=100)\n",
    "\n",
    "locations = []\n",
    "#constructing the grid with these points\n",
    "for lat in lats:\n",
    "    for lon in longs:\n",
    "        locations.append((lat, lon))\n",
    "\n",
    "locations = np.array(locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K = 5\n",
    "For 5 neightbors we find not so clearly defined clusters of crimes and some big clusters probably defined by a small number of crimes (specially in the noth west corner of the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#training for the default of K=5\n",
    "knn=neighbors.KNeighborsClassifier()\n",
    "knn.fit(gps_cords, categories)\n",
    "predictions = knn.predict(locations)\n",
    "\n",
    "#selecting the predicted crimes for each category\n",
    "drugs = np.where(predictions=='DRUG/NARCOTIC')\n",
    "prostitutions = np.where(predictions=='PROSTITUTION')\n",
    "driving = np.where(predictions=='DRIVING UNDER THE INFLUENCE')\n",
    "\n",
    "#they get colored diferently for each crime\n",
    "geo_data = {'lat': locations[:,0][drugs], 'lon': locations[:,1][drugs]}\n",
    "geoplotlib.dot(geo_data, color='red')\n",
    "geo_data = {'lat': locations[:,0][prostitutions], 'lon': locations[:,1][prostitutions]}\n",
    "geoplotlib.dot(geo_data, color='blue')\n",
    "geo_data = {'lat': locations[:,0][driving], 'lon': locations[:,1][driving]}\n",
    "geoplotlib.dot(geo_data, color='green')\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K = 10\n",
    "For 10 neighbours we can see more clearly defined clusters and we find less small groups of neighbouring predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn=neighbors.KNeighborsClassifier(10)\n",
    "knn.fit(gps_cords, categories)\n",
    "predictions = knn.predict(locations)\n",
    "\n",
    "drugs = np.where(predictions=='DRUG/NARCOTIC')\n",
    "prostitutions = np.where(predictions=='PROSTITUTION')\n",
    "driving = np.where(predictions=='DRIVING UNDER THE INFLUENCE')\n",
    "\n",
    "geo_data = {'lat': locations[:,0][drugs], 'lon': locations[:,1][drugs]}\n",
    "geoplotlib.dot(geo_data, color='red')\n",
    "geo_data = {'lat': locations[:,0][prostitutions], 'lon': locations[:,1][prostitutions]}\n",
    "geoplotlib.dot(geo_data, color='blue')\n",
    "geo_data = {'lat': locations[:,0][driving], 'lon': locations[:,1][driving]}\n",
    "geoplotlib.dot(geo_data, color='green')\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K = 30\n",
    "For 30 neighbours the number of small clusters have gone down greatly, and they are much more defined, allowing to understand better the correlation between the place and the category, in this map we are seeing 3 areas where the prostitution crimes usually occur, several places for Drug related crimes and the rest is associated with Driving under the influence which suggests that this crime is not focused in a specific area.\n",
    "We see as well that compared to the 10 neighbour map we lost several clusters of Drug related crimes, which could indicate that this map is not so precise in predicting the category of the crimes as the one with 10 neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn=neighbors.KNeighborsClassifier(30)\n",
    "knn.fit(gps_cords, categories)\n",
    "predictions = knn.predict(locations)\n",
    "\n",
    "drugs = np.where(predictions=='DRUG/NARCOTIC')\n",
    "prostitutions = np.where(predictions=='PROSTITUTION')\n",
    "driving = np.where(predictions=='DRIVING UNDER THE INFLUENCE')\n",
    "\n",
    "geo_data = {'lat': locations[:,0][drugs], 'lon': locations[:,1][drugs]}\n",
    "geoplotlib.dot(geo_data, color='red')\n",
    "geo_data = {'lat': locations[:,0][prostitutions], 'lon': locations[:,1][prostitutions]}\n",
    "geoplotlib.dot(geo_data, color='blue')\n",
    "geo_data = {'lat': locations[:,0][driving], 'lon': locations[:,1][driving]}\n",
    "geoplotlib.dot(geo_data, color='green')\n",
    "geoplotlib.set_bbox(BoundingBox(north=37.8, west=-122.5, south=37.7, east=-122.3))\n",
    "\n",
    "geoplotlib.inline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1D: Multiple regression and the Red Baron\n",
    "\n",
    "Investigate Chief Suneman's idea is that the Red Baron might pick the time of his attacks according to a pattern that we can detect using the powers of data science.\n",
    "\n",
    "If he's right, we can identify the time of the next attack, which will help us end this insanity once and for all. Well, let's see if he is right!\n",
    "\n",
    "* Start from all cases having `Red Baron` in the resolution field and use the day of the week to predict the hour of the day when he is attacking, e.g. use **linear regression** to infer the hour of the day based on the weekday! Again, take 4/5 of the data for training and then calculate goodness of fit using $R^2$ on the rest 1/5. Don't forget to rescale your input variables! (Note 1: My goodness of fit after using the weekdays is only around 0.618). (Note 2: For multivariate regression, as always you can simply re-use the code in the DSFS book (Chapters 14-15) or scikit-learn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = download_file('https://raw.githubusercontent.com/suneman/socialdataanalysis2016/master/files/x-files.csv',\n",
    "                          'x-files.csv')\n",
    "\n",
    "data = []\n",
    "with open(file_path, 'r') as infile:\n",
    "    reader = csv.DictReader(infile, delimiter=',')\n",
    "    for line in reader:\n",
    "        crime = {}\n",
    "        date = dateutil.parser.parse(line[\"Date\"]+\" \"+line[\"Time\"])\n",
    "        crime['hour'] = date.hour\n",
    "        crime['weekday'] = date.weekday()\n",
    "        crime['year'] = date.year\n",
    "        crime[\"Resolution\"] = line['Resolution']\n",
    "        crime[\"latitude\"], crime[\"longitude\"] = eval(line[\"Location\"])\n",
    "        data.append(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "red_barons = filter(lambda a: a['Resolution'] == 'RED BARON', data)\n",
    "\n",
    "X = [item['weekday'] for item in red_barons]\n",
    "Y = [item['hour'] for item in red_barons]\n",
    "\n",
    "train_indexes = np.random.choice(len(X), int(len(X) * 0.8), replace=False)\n",
    "test_indexes = np.setdiff1d(range(len(X)), train_indexes)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "train_X = X[train_indexes].reshape(-1, 1)\n",
    "test_X = X[test_indexes].reshape(-1, 1)\n",
    "train_Y = Y[train_indexes].reshape(-1, 1)\n",
    "test_Y = Y[test_indexes].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_X, train_Y)\n",
    "print('Variance score: %.5f' % regr.score(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now, add the crime year as well to the input variables! Did the goodness of fit improve? (Note: Mine did to 0.809)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2 = [item['year'] for item in red_barons]\n",
    "X2 = np.array(X2)\n",
    "        \n",
    "newX = np.concatenate([X, X2]).reshape(2, len(X)).T\n",
    "train_X = newX[train_indexes]\n",
    "test_X = newX[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_X, train_Y)\n",
    "print('Variance score: %.5f' % regr.score(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goodness of fit improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is still low. Inspired by a movie he once watched, Chief Suneman yells: \"Let's add the longitude of the crimes as well!\" Is your prediction getting better? (It should, to around 0.993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X3 = [item['longitude'] for item in red_barons]\n",
    "X3 = np.array(X3)\n",
    "\n",
    "newX = np.concatenate([X, X2, X3]).reshape(3, len(X)).T\n",
    "train_X = newX[train_indexes]\n",
    "test_X = newX[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_X, train_Y)\n",
    "print('Variance score: %.5f' % regr.score(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not seem to be any pattern in longitudes the predictions are the same and sometimes even worse "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Very nice! Why not add latitude as well? What do you find now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X4 = [item['latitude'] for item in red_barons]\n",
    "X4 = np.array(X4)\n",
    "\n",
    "newX = np.concatenate([X, X2, X3, X4]).reshape(4, len(X)).T\n",
    "train_X = newX[train_indexes]\n",
    "test_X = newX[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(train_X, train_Y)\n",
    "print('Variance score: %.5f' % regr.score(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there is a pattern in longitudes the predictions now are way better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
